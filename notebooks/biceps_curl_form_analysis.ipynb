{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ‹ï¸ Biceps Curl Form Analysis Notebook\n",
                "\n",
                "Bu notebook, **Mediapipe Pose** kullanarak biceps curl videolarÄ±ndan form analizi yapar ve **RandomForest** ile \"form doÄŸru mu yanlÄ±ÅŸ mÄ±?\" sÄ±nÄ±flandÄ±rmasÄ± gerÃ§ekleÅŸtirir.\n",
                "\n",
                "## Ä°Ã§indekiler\n",
                "1. KÃ¼tÃ¼phanelerin Kurulumu ve Import\n",
                "2. Google Drive Mount (Opsiyonel)\n",
                "3. Dataset Yolu KonfigÃ¼rasyonu\n",
                "4. YardÄ±mcÄ± Fonksiyonlar\n",
                "5. Feature Extraction Pipeline\n",
                "6. Dataset OluÅŸturma\n",
                "7. RandomForest Model EÄŸitimi\n",
                "8. DeÄŸerlendirme ve GÃ¶rselleÅŸtirme\n",
                "9. Model Kaydetme\n",
                "10. Tek Video ile Tahmin (Opsiyonel)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. KÃ¼tÃ¼phanelerin Kurulumu ve Import\n",
                "\n",
                "Gerekli kÃ¼tÃ¼phaneleri kurup import ediyoruz."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gerekli kÃ¼tÃ¼phaneleri kur\n",
                "!pip install mediapipe opencv-python numpy pandas scikit-learn tqdm matplotlib joblib -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import'lar\n",
                "import os\n",
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import mediapipe as mp\n",
                "import matplotlib.pyplot as plt\n",
                "import joblib\n",
                "import warnings\n",
                "\n",
                "from tqdm import tqdm\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    classification_report,\n",
                "    confusion_matrix\n",
                ")\n",
                "\n",
                "# UyarÄ±larÄ± kapat\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Mediapipe Pose modÃ¼lÃ¼nÃ¼ baÅŸlat\n",
                "mp_pose = mp.solutions.pose\n",
                "mp_drawing = mp.solutions.drawing_utils\n",
                "\n",
                "print(\"âœ… TÃ¼m kÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")\n",
                "print(f\"OpenCV version: {cv2.__version__}\")\n",
                "print(f\"Mediapipe version: {mp.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Google Drive Mount (Opsiyonel)\n",
                "\n",
                "EÄŸer dataset'iniz Google Drive'da ise bu hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Google Drive'Ä± baÄŸla (Colab'da Ã§alÄ±ÅŸtÄ±rÄ±n)\n",
                "# EÄŸer dataset lokal dosya sisteminde ise bu hÃ¼creyi atlayabilirsiniz.\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "print(\"âœ… Google Drive baÅŸarÄ±yla baÄŸlandÄ±!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Yolu KonfigÃ¼rasyonu\n",
                "\n",
                "Dataset'inizin kÃ¶k dizinini burada ayarlayÄ±n.\n",
                "\n",
                "**Beklenen klasÃ¶r yapÄ±sÄ±:**\n",
                "```\n",
                "dataset/\n",
                "  train/\n",
                "    true/    # iyi form biceps curl videolarÄ±\n",
                "    false/   # kÃ¶tÃ¼ form biceps curl videolarÄ±\n",
                "  test/\n",
                "    true/\n",
                "    false/\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Dataset kÃ¶k dizinini buraya girin\n",
                "# Google Drive kullanÄ±yorsanÄ±z: \"/content/drive/MyDrive/dataset\"\n",
                "# Colab'a upload ettiyseniz: \"/content/dataset\"\n",
                "\n",
                "DATA_ROOT = \"/content/dataset\"  # <-- Buraya kendi yolunuzu yazÄ±n\n",
                "\n",
                "# Video iÅŸleme parametreleri\n",
                "FRAME_SKIP = 2  # Her kaÃ§ frame'den birini iÅŸleyeceÄŸimiz (performans iÃ§in)\n",
                "MIN_FRAMES = 10  # Minimum frame sayÄ±sÄ± (daha az frame'li videolar atlanÄ±r)\n",
                "MAX_FRAMES = None  # Maksimum frame sayÄ±sÄ± (None = tÃ¼mÃ¼)\n",
                "\n",
                "# Desteklenen video formatlarÄ±\n",
                "VIDEO_EXTENSIONS = ('.mp4', '.mov', '.avi', '.mkv', '.webm', '.m4v')\n",
                "\n",
                "# Dataset yapÄ±sÄ±nÄ± kontrol et\n",
                "print(f\"ğŸ“ Dataset kÃ¶k dizini: {DATA_ROOT}\")\n",
                "print(f\"ğŸ¬ Frame skip: {FRAME_SKIP}\")\n",
                "print(f\"ğŸ“Š Minimum frame sayÄ±sÄ±: {MIN_FRAMES}\")\n",
                "\n",
                "# Dizinlerin varlÄ±ÄŸÄ±nÄ± kontrol et\n",
                "for split in ['train', 'test']:\n",
                "    for label in ['true', 'false']:\n",
                "        path = os.path.join(DATA_ROOT, split, label)\n",
                "        if os.path.exists(path):\n",
                "            video_count = len([f for f in os.listdir(path) if f.lower().endswith(VIDEO_EXTENSIONS)])\n",
                "            print(f\"  âœ… {split}/{label}: {video_count} video\")\n",
                "        else:\n",
                "            print(f\"  âš ï¸ {split}/{label}: Dizin bulunamadÄ±!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. YardÄ±mcÄ± Fonksiyonlar\n",
                "\n",
                "### 4.1 AÃ§Ä± Hesaplama Fonksiyonu\n",
                "\n",
                "ÃœÃ§ nokta arasÄ±ndaki aÃ§Ä±yÄ± hesaplar (2D ve 3D landmark'lar iÃ§in)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_angle(point_a, point_b, point_c):\n",
                "    \"\"\"\n",
                "    ÃœÃ§ nokta arasÄ±ndaki aÃ§Ä±yÄ± hesaplar.\n",
                "    \n",
                "    AÃ§Ä±, point_b noktasÄ±ndaki aÃ§Ä±dÄ±r (point_a -> point_b -> point_c).\n",
                "    \n",
                "    Args:\n",
                "        point_a: Ä°lk nokta (tuple veya array: x, y veya x, y, z)\n",
                "        point_b: KÃ¶ÅŸe noktasÄ± (aÃ§Ä± burada Ã¶lÃ§Ã¼lÃ¼r)\n",
                "        point_c: Son nokta\n",
                "    \n",
                "    Returns:\n",
                "        float: Derece cinsinden aÃ§Ä± (0-180)\n",
                "    \"\"\"\n",
                "    a = np.array(point_a)\n",
                "    b = np.array(point_b)\n",
                "    c = np.array(point_c)\n",
                "    \n",
                "    # VektÃ¶rleri hesapla\n",
                "    ba = a - b\n",
                "    bc = c - b\n",
                "    \n",
                "    # KosinÃ¼s formÃ¼lÃ¼ ile aÃ§Ä± hesapla\n",
                "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
                "    \n",
                "    # SayÄ±sal hatalarÄ± Ã¶nlemek iÃ§in -1 ile 1 arasÄ±nda sÄ±nÄ±rla\n",
                "    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
                "    \n",
                "    # Radyandan dereceye Ã§evir\n",
                "    angle = np.degrees(np.arccos(cosine_angle))\n",
                "    \n",
                "    return angle\n",
                "\n",
                "\n",
                "def calculate_vertical_angle(shoulder, hip):\n",
                "    \"\"\"\n",
                "    Omuz-kalÃ§a hattÄ±nÄ±n dikey eksene gÃ¶re aÃ§Ä±sÄ±nÄ± hesaplar (torso angle).\n",
                "    \n",
                "    Args:\n",
                "        shoulder: Omuz koordinatlarÄ± (x, y)\n",
                "        hip: KalÃ§a koordinatlarÄ± (x, y)\n",
                "    \n",
                "    Returns:\n",
                "        float: Derece cinsinden torso aÃ§Ä±sÄ±\n",
                "    \"\"\"\n",
                "    # Dikey referans noktasÄ± (omuzun tam Ã¼stÃ¼nde bir nokta)\n",
                "    vertical_point = (shoulder[0], shoulder[1] - 1.0)  # YukarÄ± doÄŸru\n",
                "    \n",
                "    return calculate_angle(vertical_point, shoulder, hip)\n",
                "\n",
                "\n",
                "# Test\n",
                "test_angle = calculate_angle((0, 0), (1, 1), (2, 0))\n",
                "print(f\"Test aÃ§Ä±sÄ± (90 derece olmalÄ±): {test_angle:.1f}Â°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Video Listeleme Fonksiyonu\n",
                "\n",
                "Train veya test klasÃ¶rÃ¼ndeki tÃ¼m videolarÄ± label'larÄ± ile birlikte listeler."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def list_videos(split_dir):\n",
                "    \"\"\"\n",
                "    Belirtilen split dizinindeki tÃ¼m videolarÄ± label'larÄ± ile listeler.\n",
                "    \n",
                "    Args:\n",
                "        split_dir: Split dizini (Ã¶rn: /content/dataset/train)\n",
                "    \n",
                "    Returns:\n",
                "        list: [(video_path, label), ...] formatÄ±nda tuple listesi\n",
                "              true klasÃ¶rÃ¼ -> label = 1 (iyi form)\n",
                "              false klasÃ¶rÃ¼ -> label = 0 (kÃ¶tÃ¼ form)\n",
                "    \"\"\"\n",
                "    videos = []\n",
                "    \n",
                "    # Label klasÃ¶rlerini tara\n",
                "    label_mapping = {'true': 1, 'false': 0}\n",
                "    \n",
                "    for label_name, label_value in label_mapping.items():\n",
                "        label_dir = os.path.join(split_dir, label_name)\n",
                "        \n",
                "        if not os.path.exists(label_dir):\n",
                "            print(f\"âš ï¸ UyarÄ±: {label_dir} dizini bulunamadÄ±!\")\n",
                "            continue\n",
                "        \n",
                "        # Video dosyalarÄ±nÄ± bul\n",
                "        for filename in os.listdir(label_dir):\n",
                "            if filename.lower().endswith(VIDEO_EXTENSIONS):\n",
                "                video_path = os.path.join(label_dir, filename)\n",
                "                videos.append((video_path, label_value))\n",
                "    \n",
                "    print(f\"ğŸ“¹ {split_dir} dizininde {len(videos)} video bulundu.\")\n",
                "    \n",
                "    return videos\n",
                "\n",
                "\n",
                "# Test - sadece dizin varsa Ã§alÄ±ÅŸacak\n",
                "if os.path.exists(os.path.join(DATA_ROOT, 'train')):\n",
                "    test_videos = list_videos(os.path.join(DATA_ROOT, 'train'))\n",
                "    print(f\"Ã–rnek: {test_videos[:2] if test_videos else 'Video bulunamadÄ±'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Extraction Pipeline\n",
                "\n",
                "### 5.1 Tek Video iÃ§in Feature Ã‡Ä±karma Fonksiyonu\n",
                "\n",
                "Bu fonksiyon, bir videodan tÃ¼m biceps curl feature'larÄ±nÄ± Ã§Ä±karÄ±r."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_features_from_video(video_path, pose, frame_skip=2, max_frames=None, min_frames=10):\n",
                "    \"\"\"\n",
                "    Tek bir videodan biceps curl iÃ§in feature'larÄ± Ã§Ä±karÄ±r.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Video dosyasÄ±nÄ±n yolu\n",
                "        pose: Mediapipe Pose nesnesi\n",
                "        frame_skip: KaÃ§ frame'de bir iÅŸleneceÄŸi (performans iÃ§in)\n",
                "        max_frames: Ä°ÅŸlenecek maksimum frame sayÄ±sÄ± (None = tÃ¼mÃ¼)\n",
                "        min_frames: Minimum frame sayÄ±sÄ± (daha azsa None dÃ¶ner)\n",
                "    \n",
                "    Returns:\n",
                "        dict: Feature dictionary veya None (hata durumunda)\n",
                "    \"\"\"\n",
                "    \n",
                "    # Video'yu aÃ§\n",
                "    cap = cv2.VideoCapture(video_path)\n",
                "    \n",
                "    if not cap.isOpened():\n",
                "        print(f\"âš ï¸ Video aÃ§Ä±lamadÄ±: {video_path}\")\n",
                "        return None\n",
                "    \n",
                "    # Video Ã¶zellikleri\n",
                "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
                "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    \n",
                "    if fps <= 0:\n",
                "        fps = 30.0  # VarsayÄ±lan FPS\n",
                "    \n",
                "    # Frame bazlÄ± Ã¶lÃ§Ã¼mler iÃ§in listeler\n",
                "    elbow_left_angles = []\n",
                "    elbow_right_angles = []\n",
                "    \n",
                "    shoulder_left_x = []\n",
                "    shoulder_left_y = []\n",
                "    shoulder_right_x = []\n",
                "    shoulder_right_y = []\n",
                "    \n",
                "    shoulder_left_angles = []  # Omuz aÃ§Ä±sÄ± (dirsek-omuz-kalÃ§a)\n",
                "    shoulder_right_angles = []\n",
                "    \n",
                "    torso_angles = []  # GÃ¶vde aÃ§Ä±sÄ± (dikey eksene gÃ¶re)\n",
                "    \n",
                "    frame_count = 0\n",
                "    processed_frames = 0\n",
                "    \n",
                "    # Landmarklar iÃ§in indeksler\n",
                "    # Sol taraf\n",
                "    LEFT_SHOULDER = mp_pose.PoseLandmark.LEFT_SHOULDER.value\n",
                "    LEFT_ELBOW = mp_pose.PoseLandmark.LEFT_ELBOW.value\n",
                "    LEFT_WRIST = mp_pose.PoseLandmark.LEFT_WRIST.value\n",
                "    LEFT_HIP = mp_pose.PoseLandmark.LEFT_HIP.value\n",
                "    \n",
                "    # SaÄŸ taraf\n",
                "    RIGHT_SHOULDER = mp_pose.PoseLandmark.RIGHT_SHOULDER.value\n",
                "    RIGHT_ELBOW = mp_pose.PoseLandmark.RIGHT_ELBOW.value\n",
                "    RIGHT_WRIST = mp_pose.PoseLandmark.RIGHT_WRIST.value\n",
                "    RIGHT_HIP = mp_pose.PoseLandmark.RIGHT_HIP.value\n",
                "    \n",
                "    while cap.isOpened():\n",
                "        ret, frame = cap.read()\n",
                "        \n",
                "        if not ret:\n",
                "            break\n",
                "        \n",
                "        frame_count += 1\n",
                "        \n",
                "        # Frame skip uygula\n",
                "        if frame_count % frame_skip != 0:\n",
                "            continue\n",
                "        \n",
                "        # Maksimum frame kontrolÃ¼\n",
                "        if max_frames and processed_frames >= max_frames:\n",
                "            break\n",
                "        \n",
                "        # RGB'ye Ã§evir (Mediapipe RGB bekler)\n",
                "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "        \n",
                "        # Pose tespiti\n",
                "        results = pose.process(frame_rgb)\n",
                "        \n",
                "        if results.pose_landmarks:\n",
                "            landmarks = results.pose_landmarks.landmark\n",
                "            \n",
                "            # Landmark koordinatlarÄ±nÄ± al\n",
                "            # Sol taraf\n",
                "            l_shoulder = [landmarks[LEFT_SHOULDER].x, landmarks[LEFT_SHOULDER].y]\n",
                "            l_elbow = [landmarks[LEFT_ELBOW].x, landmarks[LEFT_ELBOW].y]\n",
                "            l_wrist = [landmarks[LEFT_WRIST].x, landmarks[LEFT_WRIST].y]\n",
                "            l_hip = [landmarks[LEFT_HIP].x, landmarks[LEFT_HIP].y]\n",
                "            \n",
                "            # SaÄŸ taraf\n",
                "            r_shoulder = [landmarks[RIGHT_SHOULDER].x, landmarks[RIGHT_SHOULDER].y]\n",
                "            r_elbow = [landmarks[RIGHT_ELBOW].x, landmarks[RIGHT_ELBOW].y]\n",
                "            r_wrist = [landmarks[RIGHT_WRIST].x, landmarks[RIGHT_WRIST].y]\n",
                "            r_hip = [landmarks[RIGHT_HIP].x, landmarks[RIGHT_HIP].y]\n",
                "            \n",
                "            # === DÄ°RSEK AÃ‡ILARI ===\n",
                "            # Sol dirsek aÃ§Ä±sÄ± (omuz -> dirsek -> bilek)\n",
                "            left_elbow_angle = calculate_angle(l_shoulder, l_elbow, l_wrist)\n",
                "            elbow_left_angles.append(left_elbow_angle)\n",
                "            \n",
                "            # SaÄŸ dirsek aÃ§Ä±sÄ±\n",
                "            right_elbow_angle = calculate_angle(r_shoulder, r_elbow, r_wrist)\n",
                "            elbow_right_angles.append(right_elbow_angle)\n",
                "            \n",
                "            # === OMUZ KOORDÄ°NATLARI ===\n",
                "            shoulder_left_x.append(landmarks[LEFT_SHOULDER].x)\n",
                "            shoulder_left_y.append(landmarks[LEFT_SHOULDER].y)\n",
                "            shoulder_right_x.append(landmarks[RIGHT_SHOULDER].x)\n",
                "            shoulder_right_y.append(landmarks[RIGHT_SHOULDER].y)\n",
                "            \n",
                "            # === OMUZ AÃ‡ILARI (dirsek-omuz-kalÃ§a) ===\n",
                "            left_shoulder_angle = calculate_angle(l_elbow, l_shoulder, l_hip)\n",
                "            shoulder_left_angles.append(left_shoulder_angle)\n",
                "            \n",
                "            right_shoulder_angle = calculate_angle(r_elbow, r_shoulder, r_hip)\n",
                "            shoulder_right_angles.append(right_shoulder_angle)\n",
                "            \n",
                "            # === GÃ–VDE AÃ‡ISI ===\n",
                "            # Orta omuz ve orta kalÃ§a noktalarÄ±\n",
                "            mid_shoulder = [\n",
                "                (l_shoulder[0] + r_shoulder[0]) / 2,\n",
                "                (l_shoulder[1] + r_shoulder[1]) / 2\n",
                "            ]\n",
                "            mid_hip = [\n",
                "                (l_hip[0] + r_hip[0]) / 2,\n",
                "                (l_hip[1] + r_hip[1]) / 2\n",
                "            ]\n",
                "            \n",
                "            torso_angle = calculate_vertical_angle(mid_shoulder, mid_hip)\n",
                "            torso_angles.append(torso_angle)\n",
                "            \n",
                "            processed_frames += 1\n",
                "    \n",
                "    cap.release()\n",
                "    \n",
                "    # Minimum frame kontrolÃ¼\n",
                "    if processed_frames < min_frames:\n",
                "        print(f\"âš ï¸ Yetersiz frame ({processed_frames}): {video_path}\")\n",
                "        return None\n",
                "    \n",
                "    # === FEATURE'LARI HESAPLA ===\n",
                "    features = {}\n",
                "    \n",
                "    # --- Video sÃ¼resi ---\n",
                "    features['video_duration'] = total_frames / fps\n",
                "    features['processed_frames'] = processed_frames\n",
                "    \n",
                "    # --- Dirsek AÃ§Ä±sÄ± Feature'larÄ± ---\n",
                "    # Sol dirsek\n",
                "    features['elbow_left_min'] = np.min(elbow_left_angles)\n",
                "    features['elbow_left_max'] = np.max(elbow_left_angles)\n",
                "    features['elbow_left_range'] = features['elbow_left_max'] - features['elbow_left_min']\n",
                "    features['elbow_left_mean'] = np.mean(elbow_left_angles)\n",
                "    features['elbow_left_std'] = np.std(elbow_left_angles)\n",
                "    \n",
                "    # SaÄŸ dirsek\n",
                "    features['elbow_right_min'] = np.min(elbow_right_angles)\n",
                "    features['elbow_right_max'] = np.max(elbow_right_angles)\n",
                "    features['elbow_right_range'] = features['elbow_right_max'] - features['elbow_right_min']\n",
                "    features['elbow_right_mean'] = np.mean(elbow_right_angles)\n",
                "    features['elbow_right_std'] = np.std(elbow_right_angles)\n",
                "    \n",
                "    # Dirsek simetrisi\n",
                "    elbow_diffs = np.abs(np.array(elbow_left_angles) - np.array(elbow_right_angles))\n",
                "    features['elbow_lr_diff_mean'] = np.mean(elbow_diffs)\n",
                "    features['elbow_lr_diff_max'] = np.max(elbow_diffs)\n",
                "    \n",
                "    # --- Omuz Stabilitesi Feature'larÄ± ---\n",
                "    features['shoulder_left_x_std'] = np.std(shoulder_left_x)\n",
                "    features['shoulder_left_y_std'] = np.std(shoulder_left_y)\n",
                "    features['shoulder_right_x_std'] = np.std(shoulder_right_x)\n",
                "    features['shoulder_right_y_std'] = np.std(shoulder_right_y)\n",
                "    \n",
                "    # Omuz aÃ§Ä±sÄ± feature'larÄ±\n",
                "    features['shoulder_left_angle_min'] = np.min(shoulder_left_angles)\n",
                "    features['shoulder_left_angle_max'] = np.max(shoulder_left_angles)\n",
                "    features['shoulder_left_angle_range'] = features['shoulder_left_angle_max'] - features['shoulder_left_angle_min']\n",
                "    features['shoulder_left_angle_std'] = np.std(shoulder_left_angles)\n",
                "    \n",
                "    features['shoulder_right_angle_min'] = np.min(shoulder_right_angles)\n",
                "    features['shoulder_right_angle_max'] = np.max(shoulder_right_angles)\n",
                "    features['shoulder_right_angle_range'] = features['shoulder_right_angle_max'] - features['shoulder_right_angle_min']\n",
                "    features['shoulder_right_angle_std'] = np.std(shoulder_right_angles)\n",
                "    \n",
                "    # Omuz simetrisi\n",
                "    shoulder_angle_diffs = np.abs(np.array(shoulder_left_angles) - np.array(shoulder_right_angles))\n",
                "    features['shoulder_lr_diff_mean'] = np.mean(shoulder_angle_diffs)\n",
                "    \n",
                "    # --- GÃ¶vde (Torso) Stabilitesi Feature'larÄ± ---\n",
                "    features['torso_angle_mean'] = np.mean(torso_angles)\n",
                "    features['torso_angle_std'] = np.std(torso_angles)\n",
                "    features['torso_angle_range'] = np.max(torso_angles) - np.min(torso_angles)\n",
                "    features['torso_angle_min'] = np.min(torso_angles)\n",
                "    features['torso_angle_max'] = np.max(torso_angles)\n",
                "    \n",
                "    # --- AÃ§Ä±sal HÄ±z Feature'larÄ± (Kinematik) ---\n",
                "    # Sol dirsek aÃ§Ä±sal hÄ±z (frame bazlÄ± tÃ¼rev)\n",
                "    elbow_left_vel = np.diff(elbow_left_angles)\n",
                "    features['elbow_left_ang_vel_max'] = np.max(np.abs(elbow_left_vel)) if len(elbow_left_vel) > 0 else 0\n",
                "    features['elbow_left_ang_vel_std'] = np.std(elbow_left_vel) if len(elbow_left_vel) > 0 else 0\n",
                "    features['elbow_left_ang_vel_mean'] = np.mean(np.abs(elbow_left_vel)) if len(elbow_left_vel) > 0 else 0\n",
                "    \n",
                "    # SaÄŸ dirsek aÃ§Ä±sal hÄ±z\n",
                "    elbow_right_vel = np.diff(elbow_right_angles)\n",
                "    features['elbow_right_ang_vel_max'] = np.max(np.abs(elbow_right_vel)) if len(elbow_right_vel) > 0 else 0\n",
                "    features['elbow_right_ang_vel_std'] = np.std(elbow_right_vel) if len(elbow_right_vel) > 0 else 0\n",
                "    features['elbow_right_ang_vel_mean'] = np.mean(np.abs(elbow_right_vel)) if len(elbow_right_vel) > 0 else 0\n",
                "    \n",
                "    return features\n",
                "\n",
                "\n",
                "print(\"âœ… Feature extraction fonksiyonu tanÄ±mlandÄ±!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Dataset OluÅŸturma Fonksiyonu\n",
                "\n",
                "TÃ¼m videolarÄ± iÅŸleyip feature DataFrame'i oluÅŸturur."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_dataset(split_dir, frame_skip=2, max_frames=None, min_frames=10):\n",
                "    \"\"\"\n",
                "    Belirtilen split dizinindeki tÃ¼m videolardan feature'larÄ± Ã§Ä±karÄ±r.\n",
                "    \n",
                "    Args:\n",
                "        split_dir: Split dizini (Ã¶rn: /content/dataset/train)\n",
                "        frame_skip: KaÃ§ frame'de bir iÅŸleneceÄŸi\n",
                "        max_frames: Ä°ÅŸlenecek maksimum frame sayÄ±sÄ±\n",
                "        min_frames: Minimum frame sayÄ±sÄ±\n",
                "    \n",
                "    Returns:\n",
                "        X: Feature DataFrame\n",
                "        y: Label Series\n",
                "        video_paths: Video yollarÄ± listesi\n",
                "    \"\"\"\n",
                "    \n",
                "    # Video listesini al\n",
                "    videos = list_videos(split_dir)\n",
                "    \n",
                "    if len(videos) == 0:\n",
                "        print(f\"âš ï¸ HiÃ§ video bulunamadÄ±: {split_dir}\")\n",
                "        return None, None, None\n",
                "    \n",
                "    all_features = []\n",
                "    all_labels = []\n",
                "    all_paths = []\n",
                "    \n",
                "    # Mediapipe Pose baÅŸlat\n",
                "    with mp_pose.Pose(\n",
                "        static_image_mode=False,\n",
                "        model_complexity=1,\n",
                "        smooth_landmarks=True,\n",
                "        enable_segmentation=False,\n",
                "        min_detection_confidence=0.5,\n",
                "        min_tracking_confidence=0.5\n",
                "    ) as pose:\n",
                "        \n",
                "        # Her video iÃ§in feature Ã§Ä±kar\n",
                "        for video_path, label in tqdm(videos, desc=f\"Processing {os.path.basename(split_dir)}\"):\n",
                "            \n",
                "            features = extract_features_from_video(\n",
                "                video_path=video_path,\n",
                "                pose=pose,\n",
                "                frame_skip=frame_skip,\n",
                "                max_frames=max_frames,\n",
                "                min_frames=min_frames\n",
                "            )\n",
                "            \n",
                "            if features is not None:\n",
                "                all_features.append(features)\n",
                "                all_labels.append(label)\n",
                "                all_paths.append(video_path)\n",
                "    \n",
                "    if len(all_features) == 0:\n",
                "        print(f\"âš ï¸ GeÃ§erli feature Ã§Ä±karÄ±lamadÄ±: {split_dir}\")\n",
                "        return None, None, None\n",
                "    \n",
                "    # DataFrame oluÅŸtur\n",
                "    X = pd.DataFrame(all_features)\n",
                "    y = pd.Series(all_labels, name='label')\n",
                "    \n",
                "    print(f\"\\nâœ… {split_dir} dataset oluÅŸturuldu:\")\n",
                "    print(f\"   Toplam video: {len(all_features)}\")\n",
                "    print(f\"   Feature sayÄ±sÄ±: {X.shape[1]}\")\n",
                "    print(f\"   Label daÄŸÄ±lÄ±mÄ±: True={sum(y==1)}, False={sum(y==0)}\")\n",
                "    \n",
                "    return X, y, all_paths\n",
                "\n",
                "\n",
                "print(\"âœ… Dataset oluÅŸturma fonksiyonu tanÄ±mlandÄ±!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Dataset OluÅŸturma\n",
                "\n",
                "Train ve test set'lerini oluÅŸturuyoruz."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train dataset oluÅŸtur\n",
                "print(\"=\"*60)\n",
                "print(\"ğŸ“Š TRAIN DATASET OLUÅTURULUYOR...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "X_train, y_train, train_paths = build_dataset(\n",
                "    split_dir=os.path.join(DATA_ROOT, \"train\"),\n",
                "    frame_skip=FRAME_SKIP,\n",
                "    max_frames=MAX_FRAMES,\n",
                "    min_frames=MIN_FRAMES\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test dataset oluÅŸtur\n",
                "print(\"=\"*60)\n",
                "print(\"ğŸ“Š TEST DATASET OLUÅTURULUYOR...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "X_test, y_test, test_paths = build_dataset(\n",
                "    split_dir=os.path.join(DATA_ROOT, \"test\"),\n",
                "    frame_skip=FRAME_SKIP,\n",
                "    max_frames=MAX_FRAMES,\n",
                "    min_frames=MIN_FRAMES\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature'larÄ± incele\n",
                "if X_train is not None:\n",
                "    print(\"\\nğŸ“‹ TRAIN DATASET Ã–ZET:\")\n",
                "    print(f\"Shape: {X_train.shape}\")\n",
                "    print(f\"\\nFeature'lar:\")\n",
                "    for i, col in enumerate(X_train.columns, 1):\n",
                "        print(f\"  {i:2d}. {col}\")\n",
                "    \n",
                "    print(f\"\\nğŸ“Š Ä°lk 5 satÄ±r:\")\n",
                "    display(X_train.head())\n",
                "else:\n",
                "    print(\"âš ï¸ Train dataset oluÅŸturulamadÄ±!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. RandomForest Model EÄŸitimi\n",
                "\n",
                "Model tanÄ±mÄ± ve eÄŸitimi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Null kontrolÃ¼\n",
                "if X_train is None or X_test is None:\n",
                "    raise ValueError(\"âŒ Dataset oluÅŸturulamadÄ±! LÃ¼tfen DATA_ROOT yolunu ve video dosyalarÄ±nÄ± kontrol edin.\")\n",
                "\n",
                "# NaN deÄŸerleri kontrol et ve temizle\n",
                "if X_train.isnull().any().any():\n",
                "    print(\"âš ï¸ Train set'te NaN deÄŸerler var, temizleniyor...\")\n",
                "    X_train = X_train.fillna(0)\n",
                "\n",
                "if X_test.isnull().any().any():\n",
                "    print(\"âš ï¸ Test set'te NaN deÄŸerler var, temizleniyor...\")\n",
                "    X_test = X_test.fillna(0)\n",
                "\n",
                "print(f\"\\nâœ… Train set: {X_train.shape[0]} Ã¶rnek, {X_train.shape[1]} feature\")\n",
                "print(f\"âœ… Test set: {X_test.shape[0]} Ã¶rnek, {X_test.shape[1]} feature\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RandomForest modeli oluÅŸtur\n",
                "print(\"ğŸŒ³ RandomForest modeli oluÅŸturuluyor...\")\n",
                "\n",
                "rf_model = RandomForestClassifier(\n",
                "    n_estimators=200,           # AÄŸaÃ§ sayÄ±sÄ±\n",
                "    max_depth=None,             # Maksimum derinlik (None = sÄ±nÄ±rsÄ±z)\n",
                "    min_samples_split=2,        # BÃ¶lÃ¼nme iÃ§in minimum Ã¶rnek sayÄ±sÄ±\n",
                "    min_samples_leaf=1,         # Yaprakta minimum Ã¶rnek sayÄ±sÄ±\n",
                "    max_features='sqrt',        # Her bÃ¶lÃ¼nmede kullanÄ±lacak feature sayÄ±sÄ±\n",
                "    class_weight='balanced',    # Dengesiz sÄ±nÄ±flar iÃ§in aÄŸÄ±rlÄ±klandÄ±rma\n",
                "    random_state=42,            # Tekrarlanabilirlik iÃ§in\n",
                "    n_jobs=-1,                  # TÃ¼m CPU Ã§ekirdeklerini kullan\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Modeli eÄŸit\n",
                "print(\"\\nğŸ‹ï¸ Model eÄŸitiliyor...\")\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "print(\"\\nâœ… Model eÄŸitimi tamamlandÄ±!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. DeÄŸerlendirme ve GÃ¶rselleÅŸtirme\n",
                "\n",
                "Model performansÄ±nÄ± Ã¶lÃ§Ã¼yoruz."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tahmin yap\n",
                "y_pred = rf_model.predict(X_test)\n",
                "\n",
                "# Accuracy\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(\"=\"*60)\n",
                "print(\"ğŸ“Š MODEL DEÄERLENDÄ°RME SONUÃ‡LARI\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nğŸ¯ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Report\n",
                "print(\"\\nğŸ“‹ CLASSIFICATION REPORT:\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "target_names = ['KÃ¶tÃ¼ Form (False)', 'Ä°yi Form (True)']\n",
                "print(classification_report(y_test, y_pred, target_names=target_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "print(\"\\nğŸ“Š CONFUSION MATRIX:\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "cm_df = pd.DataFrame(\n",
                "    cm,\n",
                "    index=['GerÃ§ek: KÃ¶tÃ¼ Form', 'GerÃ§ek: Ä°yi Form'],\n",
                "    columns=['Tahmin: KÃ¶tÃ¼ Form', 'Tahmin: Ä°yi Form']\n",
                ")\n",
                "\n",
                "display(cm_df)\n",
                "\n",
                "# Confusion Matrix gÃ¶rselleÅŸtirme\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
                "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
                "plt.colorbar()\n",
                "\n",
                "tick_marks = np.arange(2)\n",
                "plt.xticks(tick_marks, ['KÃ¶tÃ¼ Form\\n(Predicted)', 'Ä°yi Form\\n(Predicted)'], fontsize=11)\n",
                "plt.yticks(tick_marks, ['KÃ¶tÃ¼ Form\\n(Actual)', 'Ä°yi Form\\n(Actual)'], fontsize=11)\n",
                "\n",
                "# DeÄŸerleri hÃ¼crelere yaz\n",
                "thresh = cm.max() / 2.\n",
                "for i in range(cm.shape[0]):\n",
                "    for j in range(cm.shape[1]):\n",
                "        plt.text(j, i, format(cm[i, j], 'd'),\n",
                "                 ha=\"center\", va=\"center\",\n",
                "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
                "                 fontsize=16, fontweight='bold')\n",
                "\n",
                "plt.ylabel('GerÃ§ek Label', fontsize=12)\n",
                "plt.xlabel('Tahmin Edilen Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Importance\n",
                "print(\"\\nğŸ“Š FEATURE IMPORTANCE (Ä°lk 15):\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "# Feature importance'larÄ± al\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X_train.columns,\n",
                "    'importance': rf_model.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "# Ä°lk 15 feature'Ä± gÃ¶ster\n",
                "top_features = feature_importance.head(15)\n",
                "print(top_features.to_string(index=False))\n",
                "\n",
                "# GÃ¶rselleÅŸtirme\n",
                "plt.figure(figsize=(12, 8))\n",
                "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(top_features)))\n",
                "plt.barh(range(len(top_features)), top_features['importance'].values, color=colors)\n",
                "plt.yticks(range(len(top_features)), top_features['feature'].values, fontsize=11)\n",
                "plt.xlabel('Feature Importance', fontsize=12)\n",
                "plt.title('Top 15 Feature Importance\\n(Biceps Curl Form Analysis)', fontsize=14, fontweight='bold')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train set performansÄ± (overfitting kontrolÃ¼)\n",
                "print(\"\\nğŸ“Š OVERFITTING KONTROLÃœ:\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "y_train_pred = rf_model.predict(X_train)\n",
                "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
                "\n",
                "print(f\"Train Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
                "print(f\"Test Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print(f\"Fark:           {(train_accuracy - accuracy):.4f}\")\n",
                "\n",
                "if train_accuracy - accuracy > 0.1:\n",
                "    print(\"\\nâš ï¸ Overfitting olabilir! Model regularization veya daha fazla veri gerekebilir.\")\n",
                "else:\n",
                "    print(\"\\nâœ… Model iyi genelleme yapÄ±yor gibi gÃ¶rÃ¼nÃ¼yor.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Model Kaydetme\n",
                "\n",
                "EÄŸitilmiÅŸ modeli dosyaya kaydediyoruz."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model kaydetme\n",
                "MODEL_PATH = \"biceps_curl_rf_model.joblib\"\n",
                "\n",
                "joblib.dump(rf_model, MODEL_PATH)\n",
                "print(f\"âœ… Model kaydedildi: {MODEL_PATH}\")\n",
                "\n",
                "# Feature listesini de kaydet (inference iÃ§in gerekli)\n",
                "FEATURE_LIST_PATH = \"biceps_curl_features.txt\"\n",
                "with open(FEATURE_LIST_PATH, 'w') as f:\n",
                "    for feature in X_train.columns:\n",
                "        f.write(f\"{feature}\\n\")\n",
                "\n",
                "print(f\"âœ… Feature listesi kaydedildi: {FEATURE_LIST_PATH}\")\n",
                "\n",
                "# Model boyutunu gÃ¶ster\n",
                "import os\n",
                "model_size = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
                "print(f\"ğŸ“¦ Model boyutu: {model_size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Google Drive'a model kopyalama (opsiyonel)\n",
                "# Bu hÃ¼creyi Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce Drive'Ä± mount ettiÄŸinizden emin olun\n",
                "\n",
                "# TODO: Ä°stediÄŸiniz Drive yolunu ayarlayÄ±n\n",
                "DRIVE_MODEL_PATH = \"/content/drive/MyDrive/models/biceps_curl_rf_model.joblib\"\n",
                "\n",
                "try:\n",
                "    import shutil\n",
                "    os.makedirs(os.path.dirname(DRIVE_MODEL_PATH), exist_ok=True)\n",
                "    shutil.copy(MODEL_PATH, DRIVE_MODEL_PATH)\n",
                "    print(f\"âœ… Model Google Drive'a kopyalandÄ±: {DRIVE_MODEL_PATH}\")\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ Drive'a kopyalama baÅŸarÄ±sÄ±z: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Tek Video ile Tahmin (Opsiyonel)\n",
                "\n",
                "Yeni bir video Ã¼zerinde model tahmini yapmak iÃ§in kullanÄ±labilir."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_on_single_video(video_path, model, feature_columns, frame_skip=2):\n",
                "    \"\"\"\n",
                "    Tek bir video Ã¼zerinde form tahmini yapar.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Video dosyasÄ±nÄ±n yolu\n",
                "        model: EÄŸitilmiÅŸ RandomForest modeli\n",
                "        feature_columns: Feature sÄ±ralamasÄ± (list)\n",
                "        frame_skip: KaÃ§ frame'de bir iÅŸleneceÄŸi\n",
                "    \n",
                "    Returns:\n",
                "        dict: Tahmin sonuÃ§larÄ±\n",
                "    \"\"\"\n",
                "    \n",
                "    print(f\"\\nğŸ¬ Video analiz ediliyor: {os.path.basename(video_path)}\")\n",
                "    print(\"-\"*50)\n",
                "    \n",
                "    # Feature Ã§Ä±kar\n",
                "    with mp_pose.Pose(\n",
                "        static_image_mode=False,\n",
                "        model_complexity=1,\n",
                "        smooth_landmarks=True,\n",
                "        enable_segmentation=False,\n",
                "        min_detection_confidence=0.5,\n",
                "        min_tracking_confidence=0.5\n",
                "    ) as pose:\n",
                "        \n",
                "        features = extract_features_from_video(\n",
                "            video_path=video_path,\n",
                "            pose=pose,\n",
                "            frame_skip=frame_skip,\n",
                "            max_frames=None,\n",
                "            min_frames=5\n",
                "        )\n",
                "    \n",
                "    if features is None:\n",
                "        print(\"âŒ Feature Ã§Ä±karÄ±lamadÄ±!\")\n",
                "        return None\n",
                "    \n",
                "    # DataFrame'e Ã§evir (model beklediÄŸi sÄ±rada)\n",
                "    X = pd.DataFrame([features])[feature_columns]\n",
                "    \n",
                "    # NaN kontrolÃ¼\n",
                "    X = X.fillna(0)\n",
                "    \n",
                "    # Tahmin yap\n",
                "    prediction = model.predict(X)[0]\n",
                "    probabilities = model.predict_proba(X)[0]\n",
                "    \n",
                "    # SonuÃ§larÄ± yazdÄ±r\n",
                "    result = {\n",
                "        'prediction': prediction,\n",
                "        'label': 'Ä°yi Form (True)' if prediction == 1 else 'KÃ¶tÃ¼ Form (False)',\n",
                "        'confidence_good': probabilities[1],\n",
                "        'confidence_bad': probabilities[0],\n",
                "        'features': features\n",
                "    }\n",
                "    \n",
                "    print(f\"\\nğŸ“Š TAHMÄ°N SONUCU:\")\n",
                "    print(f\"   â¤ Form: {result['label']}\")\n",
                "    print(f\"   â¤ Ä°yi Form OlasÄ±lÄ±ÄŸÄ±: {result['confidence_good']*100:.1f}%\")\n",
                "    print(f\"   â¤ KÃ¶tÃ¼ Form OlasÄ±lÄ±ÄŸÄ±: {result['confidence_bad']*100:.1f}%\")\n",
                "    \n",
                "    # Ã–nemli metrikler\n",
                "    print(f\"\\nğŸ“ˆ Ã–NEMLÄ° METRÄ°KLER:\")\n",
                "    print(f\"   â¤ Sol Dirsek ROM: {features['elbow_left_range']:.1f}Â°\")\n",
                "    print(f\"   â¤ SaÄŸ Dirsek ROM: {features['elbow_right_range']:.1f}Â°\")\n",
                "    print(f\"   â¤ Torso AÃ§Ä±sÄ± (ort): {features['torso_angle_mean']:.1f}Â°\")\n",
                "    print(f\"   â¤ Omuz Stabilitesi (std): {features['shoulder_left_y_std']:.4f}\")\n",
                "    \n",
                "    return result\n",
                "\n",
                "\n",
                "print(\"âœ… Tek video tahmin fonksiyonu tanÄ±mlandÄ±!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ã–rnek kullanÄ±m: Tek video Ã¼zerinde tahmin\n",
                "# TODO: Test etmek istediÄŸiniz video yolunu buraya girin\n",
                "\n",
                "# SAMPLE_VIDEO_PATH = \"/content/test_video.mp4\"\n",
                "# result = predict_on_single_video(\n",
                "#     video_path=SAMPLE_VIDEO_PATH,\n",
                "#     model=rf_model,\n",
                "#     feature_columns=X_train.columns.tolist(),\n",
                "#     frame_skip=2\n",
                "# )\n",
                "\n",
                "print(\"ğŸ’¡ Tek video tahmini iÃ§in yukarÄ±daki hÃ¼credeki yorum satÄ±rlarÄ±nÄ± kaldÄ±rÄ±n.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“ Ã–zet\n",
                "\n",
                "Bu notebook'ta ÅŸunlarÄ± gerÃ§ekleÅŸtirdik:\n",
                "\n",
                "1. **Mediapipe Pose** ile biceps curl videolarÄ±ndan pose landmark'larÄ± Ã§Ä±kardÄ±k\n",
                "2. Frame bazlÄ± aÃ§Ä± ve postÃ¼r **feature'larÄ±** hesapladÄ±k:\n",
                "   - Dirsek aÃ§Ä±larÄ± (min, max, range, mean, std)\n",
                "   - Omuz stabilitesi (x/y koordinat varyasyonlarÄ±)\n",
                "   - GÃ¶vde aÃ§Ä±sÄ± (torso stability)\n",
                "   - AÃ§Ä±sal hÄ±z (kinematik analiz)\n",
                "3. Her video iÃ§in bu feature'larÄ± Ã¶zetleyip tek bir feature vektÃ¶rÃ¼ oluÅŸturduk\n",
                "4. **RandomForestClassifier** ile model eÄŸittik\n",
                "5. Test set Ã¼zerinde performansÄ± Ã¶lÃ§tÃ¼k:\n",
                "   - Accuracy\n",
                "   - Classification Report\n",
                "   - Confusion Matrix\n",
                "6. **Feature Importance** analizi yaptÄ±k\n",
                "7. Modeli `.joblib` formatÄ±nda kaydettik\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸš€ Ä°leriye DÃ¶nÃ¼k Ä°yileÅŸtirmeler\n",
                "\n",
                "- [ ] Rep segmentasyonu ekleyerek her rep iÃ§in ayrÄ± analiz\n",
                "- [ ] Cross-validation ile daha gÃ¼venilir performans tahmini\n",
                "- [ ] Hyperparameter tuning (GridSearchCV)\n",
                "- [ ] FarklÄ± modeller deneme (XGBoost, LightGBM, Neural Network)\n",
                "- [ ] Data augmentation (video rotation, flipping)\n",
                "\n",
                "---\n",
                "\n",
                "**ğŸ‹ï¸ Ä°yi antrenmanlar!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}